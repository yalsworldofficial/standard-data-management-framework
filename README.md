# **Standard Data Management Framework (SDMF)**

A **modular, scalable, and Python-based Data Management Framework** designed to standardize **data ingestion, validation, transformation, metadata handling, and storage** across enterprise workflows.

This framework eliminates repetitive boilerplate and provides a **consistent structure for building reliable, maintainable data pipelines**.

***

## âœ… **Key Features**

* **Modular Design** â€“ Plug-and-play components for ingestion, validation, transformation, and storage.
* **Schema Alignment & Partitioning** â€“ Built-in support for CDC (Change Data Capture) and MERGE operations.
* **Metadata Management** â€“ Centralized handling of feed specifications and lineage.
* **Scalable** â€“ Works seamlessly with **Spark**, **Delta Lake**, and distributed environments like **Databricks**.
* **Logging & Monitoring** â€“ Custom logging with retention and rotation policies.

***

## ğŸ“‚ **Project Structure**

    sdmf/
    â”œâ”€â”€ cli/                # Command-line interface for orchestration
    â”œâ”€â”€ config/             # Configurations (logging, paths, retention)
    â”œâ”€â”€ orchestrator/       # Pipeline orchestration logic
    â”œâ”€â”€ result_generator/   # Excel/Report generation utilities
    â”œâ”€â”€ utils/              # Helper functions
    â””â”€â”€ ...

***

## âš™ï¸ **Installation**

### **Option 1 (Recommended): Editable Install**

From the project root (where `pyproject.toml` is located):

```bash
pip install -e .
python -m build
```

Then run:

```bash
python -m sdmf.cli.main
```

***

## ğŸ”— **Dependencies**

Install required packages:

```bash
pip install pyspark==3.5.1 delta-spark==3.1.0
```

***

## ğŸš€ **Usage**

Run the main orchestrator:

```bash
python -m sdmf.cli.main --config config/config.ini --run_id <unique_run_id>
```

***

## ğŸ›  **Configuration**

Update `config.ini`:

```ini
[DEFAULT]
outbound_directory_name=sdmf_outbound
log_directory_name=sdmf_logs
temp_log_location=/tmp/
file_hunt_path=/dbfs/FileStore/sdmf/
log_retention_policy_in_days=7

[FILES]
master_spec_name=master_specs.xlsx
```

***

## âœ… **Logging**

* Logs are first written to `/tmp/sdmf_logs` for speed.
* After job completion, logs are moved to the final directory (`file_hunt_path`).
* Automatic cleanup of logs older than **7 days**.

***

## âœ… **Best Practices**

* Use **editable install** for development.
* Keep configs modular for different environments (Dev, QA, Prod).
* Ensure **DBFS or UC volumes** for persistent storage in Databricks.

***

## ğŸ“Œ **Next Steps**

* Add **unit tests** for core modules.
* Integrate **structured logging** (JSON) for ELK/Splunk.
* Enable **compression for archived logs**.

***
